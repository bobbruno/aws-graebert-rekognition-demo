{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Relevant Object in Construction Site Pictures\n",
    "\n",
    "This notebook demos [Amazon Rekognition Custom Labels](https://aws.amazon.com/rekognition/)' ability to learn how to detect relevant objects from pictures, using a dataset copied from [Context-based information generation from construction site images using unmanned aerial vehicle (UAV)-acquired data and image captioning](https://data.mendeley.com/datasets/4h68fmktwh/1). The dataset is licensed as [CC BY 4.0](http://creativecommons.org/licenses/by/4.0).\n",
    "\n",
    "The notebook consists of the following steps:\n",
    "\n",
    "1. Download and decompress the dataset\n",
    "2. Load all image metadata and generate classes for each bounding box:\n",
    "    1. Process the manifest file provided, consisting of the format explained on the [README](https://github.com/bobbruno/aws-rekognition-cl-demo) in the repository;\n",
    "    2. Run the bounding boxes' descriptive phrases through [Amazon Comprehend](https://aws.amazon.com/comprehend/), to identify the syntactic elements.\n",
    "    3. Extract the relevant elements from the description and process them (filtering irrelevant words/typos, lowercase, eliminating plurals) to be suitable object classes.\n",
    "3. Split the data into training and test.\n",
    "4. Generate an [Object Detection Ground Truth Manifest file](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cd-required-fields.html) describing the dataset, which is required by Rekognition Custom Labels.\n",
    "5. Create a Rekognition Project and Start the project version (model) training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup/Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image as Imagem\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"sagemaker-us-east-1-113147044314\"\n",
    "input_path = \"cl-demo/input\"\n",
    "reko_manifest_path = \"cl-demo/reko-input\"\n",
    "output_path = \"cl-demo/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dowload/Decompress the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is actually stored in S3, so we just use the boto3 S3 interface to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(\"md-datasets-cache-zipfiles-prod\", \"4h68fmktwh-1.zip\", \"data/construction.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/construction.zip -d data\n",
    "!unzip data/image_with_captions.zip -d data\n",
    "!rm data/construction.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Classifications from the Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Classes for Processing the original manifest and generating object classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox(object):\n",
    "    def __init__(self, x, y, width, height):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "class Token(object):\n",
    "    def __init__(self, TokenId, Text, BeginOffset, EndOffset, Tag, Score):\n",
    "        assert Score > 0.9, 'Not enough confidence on tag'\n",
    "        self.TokenId = TokenId\n",
    "        self.Text = Text\n",
    "        self.LowerText = Text.lower()\n",
    "        self.BeginOffset = BeginOffset\n",
    "        self.EndOffset = EndOffset\n",
    "        self.Tag = Tag\n",
    "        self.Score = Score\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "        \n",
    "class ImageAnnotation(object):\n",
    "    COLORS = {'blue', 'red', 'yellow', 'green', 'white', 'silver', 'gray', 'grey', 'orange', 'black', 'brown', 'pink', 'purple', 'while'}\n",
    "    STOPWORDS = {'elevens', 'fivr', 'plie', 'thirteens', 'pile', 'piles'}\n",
    "    \n",
    "    def __init__(self, index, bounding_box, phrase, comprehend=None):\n",
    "        self.bb = bounding_box\n",
    "        self.index = index\n",
    "        self.phrase = phrase\n",
    "        if (comprehend):\n",
    "            self.update_tokens(comprehend)\n",
    "        else:\n",
    "            self.Tokens = None\n",
    "            self.obj_class = None\n",
    "        \n",
    "    def _gen_tokens(self, comprehend):\n",
    "        return(comprehend.detect_syntax(LanguageCode='en', Text=self.phrase)['SyntaxTokens'])\n",
    "    \n",
    "    def update_tokens(self, comprehend):\n",
    "        self.Tokens = self._gen_tokens(comprehend) if len(self.phrase) else None\n",
    "        if self.Tokens and len(self.Tokens):\n",
    "            self.obj_class = self.get_class()\n",
    "        else:\n",
    "            self.obj_class = None\n",
    "        \n",
    "    def get_class(self):\n",
    "        assert self.Tokens, \"No tokenization available for class extraction\"\n",
    "        def get_object(tokens):\n",
    "            noun_part = []\n",
    "            for token in tokens:\n",
    "                if token['PartOfSpeech']['Tag'] == 'VERB':\n",
    "                    break\n",
    "                if token['Text'].lower() in ImageAnnotation.COLORS:\n",
    "                    continue\n",
    "                if token['Text'].lower() in ImageAnnotation.STOPWORDS:\n",
    "                    continue\n",
    "                if token['PartOfSpeech']['Tag'] in ('ADJ', 'NOUN', 'PROPN'):\n",
    "                    noun_part.append(token)\n",
    "            return ' '.join([x['Text'] for x in noun_part])\n",
    "        result = get_object(self.Tokens)\n",
    "        if len(result) > 0:\n",
    "            result = result[:-1] if result[-1] == 's' else result\n",
    "        return(result)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "\n",
    "class Image(object):\n",
    "    def __init__(self, path, filename, size, annotations):\n",
    "        self.path = path\n",
    "        self.filename = filename\n",
    "        self.size = size\n",
    "        self.annotations = annotations\n",
    "        im = Imagem.open(f\"{self.path}/{self.filename}\")\n",
    "        self.width, self.height = im.size\n",
    "        self.classify_image()\n",
    "        \n",
    "    def classify_image(self):\n",
    "        self.obj_classes = {annotation.obj_class for annotation in self.annotations if annotation.obj_class}\n",
    "    \n",
    "    def gen_tokens(self, comprehend):\n",
    "        for annot in self.annotations:\n",
    "            annot.update_tokens(comprehend)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function to process the manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, manifest_file, comprehend=None):\n",
    "    with open(manifest_file, \"r\") as source_manifest:\n",
    "        manifest = json.load(source_manifest)\n",
    "    images = []\n",
    "    for _, image in manifest.items():\n",
    "        try:\n",
    "            try:\n",
    "                annotations = []\n",
    "                for (index, annotation) in image['regions'].items():\n",
    "                    shape_attrs = annotation['shape_attributes']\n",
    "                    shape_attrs.pop('name')\n",
    "                    phrase = annotation['region_attributes']['phrase']\n",
    "                    img = ImageAnnotation(index, BoundingBox(**shape_attrs), phrase, comprehend)\n",
    "                    if img.obj_class and len(img.obj_class):\n",
    "                        annotations.append(img)\n",
    "            except:\n",
    "                pprint(annotation['shape_attributes'])\n",
    "                raise\n",
    "            images.append(\n",
    "                Image(path, image['filename'], image['size'], annotations)\n",
    "            )\n",
    "        except:\n",
    "            pprint(image)\n",
    "            raise\n",
    "    return(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "image_objs = load_images(\"data/UAVdata\", \"data/via_region_data_final.json\", comprehend)\n",
    "[image.classify_image() for image in image_objs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(image_objs, pct_train=0.5, random_state=None):\n",
    "    classified_images = [image for image in image_objs if len(image.obj_classes) > 0]\n",
    "    unclassified_images = [image for image in image_objs if len(image.obj_classes) == 0]\n",
    "    train_images, test_images = train_test_split(classified_images, train_size=pct_train, random_state=random_state)\n",
    "    return(train_images, test_images, unclassified_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, unclassified = split_data(image_objs, pct_train=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Ground Truth Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a mapping of the most common classes to ids (required by the manifest format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_map = {classif: i+1 for (i, (classif, _)) in\n",
    "             enumerate(\n",
    "                 Counter(\n",
    "                     [an.obj_class for image in train_data for an in image.annotations if an.obj_class and len(an.obj_class)]\n",
    "                 ).most_common(34)\n",
    "             )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_manifest(image, class_map, s3_path, job_number=0, max_annotations=None, indent=None):\n",
    "    img_annotations = image.annotations if max_annotations is None else image.annotations[:max_annotations]\n",
    "    class_map['others'] = 0\n",
    "\n",
    "    classes_in_img = {an.obj_class for an in img_annotations}\n",
    "    annotations = [{\n",
    "        \"class_id\": class_map.get(an.obj_class, 0),\n",
    "        \"left\": an.bb.x,\n",
    "        \"top\": an.bb.y,\n",
    "        \"width\": an.bb.width,\n",
    "        \"height\": an.bb.height\n",
    "    } for an in img_annotations]\n",
    "    \n",
    "    manifest = {\n",
    "        \"source-ref\": f'{s3_path}/{image.filename}',\n",
    "        \"bounding-box\": { \n",
    "            \"image_size\": [\n",
    "                {\n",
    "                    \"width\": image.width,\n",
    "                    \"height\": image.height,\n",
    "                    \"depth\": 3 \n",
    "                }\n",
    "            ],\n",
    "            \"annotations\": annotations\n",
    "        },\n",
    "        \"bounding-box-metadata\": { \n",
    "            \"objects\": [ {\"confidence\": 0.9} for _ in img_annotations ],\n",
    "            \"class-map\": {v: k for k, v in class_map.items() if k in classes_in_img},\n",
    "            \"type\": \"groundtruth/object-detection\",\n",
    "            \"human-annotated\": \"yes\",\n",
    "            \"creation-date\": \"2020-02-25T22:50:05.0000z\",\n",
    "            \"job-name\": f\"identify-construction-objs-{job_number}\"\n",
    "        }\n",
    "     }\n",
    "    return(json.dumps(manifest, indent=indent))\n",
    "\n",
    "def save_manifest(data, path):\n",
    "    s3_path = f\"s3://{bucket}/{input_path}\"\n",
    "    with open(path, 'w') as outfile:\n",
    "        for example in data:\n",
    "            outfile.write(f\"{gen_manifest(example, class_map, s3_path, max_annotations=5)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Manifest name: s3://sagemaker-us-east-1-113147044314/cl-demo/reko-input/train_manifest_5_limit_2.json\n",
      "Test Manifest name: s3://sagemaker-us-east-1-113147044314/cl-demo/reko-input/test_manifest_5_limit_2.json\n"
     ]
    }
   ],
   "source": [
    "manifest_ext = \"_5_limit_2\"\n",
    "print(f\"Train Manifest name: s3://{bucket}/{reko_manifest_path}/train_manifest{manifest_ext}.json\")\n",
    "print(f\"Test Manifest name: s3://{bucket}/{reko_manifest_path}/test_manifest{manifest_ext}.json\")\n",
    "#contruction-objects-train-max5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_manifest(train_data, f'data/train_manifest{manifest_ext}.json')\n",
    "save_manifest(test_data, f'data/test_manifest{manifest_ext}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Manifest File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Schemas:\n",
    "    \"\"\"\n",
    "    Schema Definitions\n",
    "    \"\"\"\n",
    "\n",
    "    SOURCE_SCHEMA = {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "            \"source-ref\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "            \"source-ref\": {\n",
    "                \"description\": \"Image S3 image URL\",\n",
    "                \"type\": \"string\",\n",
    "                \"pattern\": \"^s3://([^/]+)/(.*)$\"\n",
    "            }\n",
    "        },\n",
    "        \"additionalProperties\": False,\n",
    "        'maxItems': 1,\n",
    "        'minItems': 1\n",
    "    }\n",
    "\n",
    "    ATTRIBUTE_SCHEMA = {\n",
    "        \"definitions\": {\n",
    "            \"image_size\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [\n",
    "                        \"width\",\n",
    "                        \"height\",\n",
    "                        \"depth\"\n",
    "                    ],\n",
    "                    \"properties\": {\n",
    "                        \"width\": {\n",
    "                            \"$ref\": \"#/definitions/image_dimension\"\n",
    "                        },\n",
    "                        \"height\": {\n",
    "                            \"$ref\": \"#/definitions/image_dimension\"\n",
    "                        },\n",
    "                        \"depth\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"min\": 1,\n",
    "                            \"max\": 3\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"annotations\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [\n",
    "                        \"width\",\n",
    "                        \"height\",\n",
    "                        \"top\",\n",
    "                        \"left\",\n",
    "                        \"class_id\"\n",
    "                    ],\n",
    "                    \"properties\": {\n",
    "                        \"width\": {\n",
    "                            \"type\": \"number\"\n",
    "                        },\n",
    "                        \"height\": {\n",
    "                            \"type\": \"number\"\n",
    "                        },\n",
    "                        \"top\": {\n",
    "                            \"type\": \"number\"\n",
    "                        },\n",
    "                        \"left\": {\n",
    "                            \"type\": \"number\"\n",
    "                        },\n",
    "                        \"class_id\": {\n",
    "                            \"type\": \"integer\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"image_dimension\": {\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 64,\n",
    "                \"maximum\": 4096\n",
    "            },\n",
    "            \"confidence\": {\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 0.0,\n",
    "                \"maximum\": 1.0\n",
    "            },\n",
    "            \"objects\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [\n",
    "                        \"confidence\"\n",
    "                    ],\n",
    "                    \"properties\": {\n",
    "                        \"confidence\": {\n",
    "                            \"$ref\": \"#/definitions/confidence\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"patternProperties\": {\n",
    "            \"^[A-Za-z0-9-]+\": {\n",
    "                \"oneOf\": [\n",
    "                    {\n",
    "                        \"type\": \"object\",\n",
    "                        \"description\": \"Detection label data\",\n",
    "                        \"properties\": {\n",
    "                            \"annotations\": {\n",
    "                                \"$ref\": \"#/definitions/annotations\"\n",
    "                            },\n",
    "                            \"image_size\": {\n",
    "                                \"$ref\": \"#/definitions/image_size\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"annotations\",\n",
    "                            \"image_size\"\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"description\": \"Classification label data\",\n",
    "                        \"type\": [\"integer\", \"string\"]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'minItems': 1\n",
    "    }\n",
    "\n",
    "    METADATA_SCHEMA = {\n",
    "        \"definitions\": {\n",
    "            \"confidence\": {\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 0.0,\n",
    "                \"maximum\": 1.0\n",
    "            },\n",
    "            \"class-map\": {\n",
    "                \"type\": \"object\",\n",
    "                \"propertyNames\": {\"pattern\": \"^[0-9]+\"},\n",
    "                \"patternProperties\": {\"\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 256}}\n",
    "            },\n",
    "            \"objects\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [\n",
    "                        \"confidence\"\n",
    "                    ],\n",
    "                    \"properties\": {\n",
    "                        \"confidence\": {\n",
    "                            \"$ref\": \"#/definitions/confidence\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"patternProperties\": {\n",
    "            \"^[A-Za-z0-9-]+\": {\n",
    "                \"description\": \"Ground Truth label metadata\",\n",
    "                \"properties\": {\n",
    "                    \"class-map\": {\n",
    "                        \"$ref\": \"#/definitions/class-map\"\n",
    "                    },\n",
    "                    \"human-annotated\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"yes\", \"no\"]\n",
    "                    },\n",
    "                    \"type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"groundtruth/image-classification\", \"groundtruth/object-detection\"]\n",
    "                    },\n",
    "                    \"creation-date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"format\": \"date-time\"\n",
    "                    },\n",
    "                    \"confidence\": {\n",
    "                        \"$ref\": \"#/definitions/confidence\"\n",
    "                    },\n",
    "                    \"class-name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"minLength\": 1,\n",
    "                        \"maxLength\": 256\n",
    "                    },\n",
    "                    \"objects\": {\n",
    "                        \"$ref\": \"#/definitions/objects\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"human-annotated\",\n",
    "                    \"type\",\n",
    "                    \"creation-date\"\n",
    "                ],\n",
    "                \"oneOf\": [\n",
    "                    {\n",
    "                        \"required\": [\n",
    "                            \"confidence\",\n",
    "                            \"class-name\"\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"required\": [\n",
    "                            \"class-map\",\n",
    "                            \"objects\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'minItems': 5\n",
    "    }\n",
    "\n",
    "AUTO_ML_SERVICE_INPUT_BUCKET = \"rekognition-automl-training\"\n",
    "AUTO_ML_SERVICE_ARTIFACTS_BUCKET = \"rekognition-automl-models\"\n",
    "AUTO_ML_MODEL_ZOO_ARTIFACTS_BUCKET = \"rekognition-automl-modelzoo\"\n",
    "\n",
    "# Lambda input event constants\n",
    "ACCOUNT_ID = \"AccountId\"\n",
    "PROJECT_ID = \"ProjectId\"\n",
    "VERSION_NAME = \"VersionName\"\n",
    "PROJECT_VERSION_ARN = \"ProjectVersionArn\"\n",
    "S3_PROJECT_PREFIX = \"S3ProjectVersionPrefix\"\n",
    "HPO_JOB_NAME_OVERWRITE = \"HPOJobName\"\n",
    "USE_CASE = \"UseCase\"\n",
    "EXECUTION_START_TIME = \"executionStartTime\"\n",
    "\n",
    "# Lambda input context constants\n",
    "REGION = \"region\"\n",
    "SERVICE_ACCOUNT_ID = \"service_account_id\"\n",
    "VERSION = \"version\"\n",
    "\n",
    "# GT manifest fields from API definition\n",
    "DATASETS_KEY = \"Datasets\"\n",
    "USE_TYPE_KEY = \"UseType\"\n",
    "GT_MANIFEST_KEY = \"GroundTruthManifest\"\n",
    "S3_OBJECT_KEY = \"S3Object\"\n",
    "TRAIN_MANIFEST_TYPE = \"TRAINING\"\n",
    "TEST_MANIFEST_TYPE = \"TESTING\"\n",
    "OUTPUT_CONFIG_OWNER_ID_KEY = \"OutputConfigOwnerId\"\n",
    "TRAIN_DATASET_OWNER_ID_KEY = \"TrainDatasetOwnerId\"\n",
    "TEST_DATASET_OWNER_ID_KEY = \"TestDatasetOwnerId\"\n",
    "\n",
    "# Sagemaker GT Augmented manifest constants\n",
    "SOURCE_REF = \"source-ref\"\n",
    "ORIGINAL_SOURCE = \"original-source\"\n",
    "METADATA_SUFFIX = \"metadata\"\n",
    "ATTRIBUTE_ANNOTATIONS = \"annotations\"\n",
    "ATTRIBUTE_IMAGE_SIZE = \"image_size\"\n",
    "ATTRIBUTE_IMAGE_WIDTH = \"width\"\n",
    "ATTRIBUTE_IMAGE_HEIGHT = \"height\"\n",
    "ATTRIBUTE_IMAGE_LEFT = \"left\"\n",
    "ATTRIBUTE_IMAGE_TOP = \"top\"\n",
    "METADATA_CLASS_MAP = \"class-map\"\n",
    "METADATA_CLASS_NAME = \"class-name\"\n",
    "METADATA_CLASS_ID = \"class_id\"\n",
    "METADATA_OBJECTS = \"objects\"\n",
    "METADATA_HUMAN_ANNOTATED = \"human-annotated\"\n",
    "METADATA_CONFIDENCE = \"confidence\"\n",
    "METADATA_JOB_NAME = \"job-name\"\n",
    "METADATA_TYPE = \"type\"\n",
    "METADATA_CREATION_DATE = \"creation-date\"\n",
    "LABEL_TYPE_CLASSIFICATION = \"image-classification\"\n",
    "LABEL_TYPE_DETECTION = \"object-detection\"\n",
    "GROUND_TRUTH_TYPE_PREFIX = \"groundtruth/\"\n",
    "\n",
    "# Intermediate SM training manifest constants\n",
    "LABEL = \"label\"  # validated output attribute for SM training\n",
    "ANNOTATION_TYPE = \"annotation-type\"  # use type for SM training\n",
    "LINE_INDEX = \"line-idx\"  # line number from original manifest\n",
    "IMAGE_RESIZE_RATIO = \"resize-ratio\"\n",
    "\n",
    "# Output GT manifest constants\n",
    "AUTOML_OBJECT_ID = \"rekognition-custom-labels-object-id\"\n",
    "AUTOML_JOB_NAME = \"rekognition-custom-labels-training-job\"\n",
    "AUTOML_LABEL = \"rekognition-custom-labels-training\"\n",
    "\n",
    "# File constants\n",
    "JSON_EXT = \".json\"\n",
    "CSV_EXT = \".csv\"\n",
    "UTF_8 = \"utf-8\"\n",
    "\n",
    "# Manifest file\n",
    "# Manifest format reference: https://quip-amazon.com/tkHTAzxM91P7/AutoML-Manifest-Format\n",
    "TRAINING_MANIFEST = TRAIN_MANIFEST_TYPE + JSON_EXT\n",
    "TESTING_MANIFEST = TEST_MANIFEST_TYPE + JSON_EXT\n",
    "VALIDATED_GT_MANIFEST_SUFFIX = \"_gt\"  # validated GT-compatible manifest for customer\n",
    "VALIDATED_MANIFEST_STATS_SUFFIX = '_stats'  # stats required for technique selection and evaluation\n",
    "VALIDATED_MANIFEST_STATS_AFTER_FILTER_SUFFIX = '_stats_after_filter'  # stats after filter dataset\n",
    "PRESPLIT_MANIFEST = \"_presplit\"  # original training manifest (before auto-split)\n",
    "SM_MANIFEST = \"SageMakerManifest\"\n",
    "GT_MANIFEST = \"GroundTruthManifest\"\n",
    "VALID_SUFFIX = \"_valid\"  # suffix for manifest files after passing initial syntax and image validation checks\n",
    "SPLIT_SUFFIX = \"_split\"  # suffix for manifest files after auto-split step. Does not imply if dataset was autosplit\n",
    "MIN_DATASET_CLASSES = 2\n",
    "MAX_DATASET_CLASSES = 250\n",
    "\n",
    "# S3 constants\n",
    "S3_PREFIX = \"s3://\"\n",
    "CONTENT_LENGTH = \"ContentLength\"\n",
    "CHUNK_SIZE_IN_BYTES = 200*1024*1024  # batch size for streaming s3 download\n",
    "MIN_MULTIPART_UPLOAD_CHUNK_SIZE = 5*1024*1024  # 5MB\n",
    "MAX_MULTIPART_UPLOAD_CHUNK_SIZE = 5*1024*1024*1024  # 5GB\n",
    "MOCK_S3_BUCKET_OWNER_ID = '75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a'  # This is fixed owner-id\n",
    "# returned by mock_s3 used in unit tests.\n",
    "\n",
    "# Validation constants\n",
    "MAX_DATASET_OBJECTS = 250000\n",
    "ONE_GB_IN_BYTES = 1*1024*1024*1024\n",
    "MAX_MANIFEST_FILE_SIZE_IN_BYTES = ONE_GB_IN_BYTES\n",
    "MAX_IMAGE_SIZE_IN_BYTES = 15*1024*1024  # 15MB\n",
    "MAX_IMAGE_DIMENSION = 4096  # max supported image size\n",
    "MIN_IMAGE_DIMENSION = 64  # min supported image size\n",
    "RESIZE_IMAGE_DIMENSION = 1280  # image dimension re-size threshold\n",
    "MAX_RESIZED_IMAGE_IN_BYTES = RESIZE_IMAGE_DIMENSION * RESIZE_IMAGE_DIMENSION * 3  # 1-byte per plane for RGB\n",
    "S3_VERSION = \"Version\"\n",
    "JPEG_QUALITY = 100\n",
    "\n",
    "# Training data related constants\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "MIN_EXPECTED_NUM_CLASS_LABELS = 2\n",
    "MAX_BBS_PER_IMAGE = 50\n",
    "MAX_DATASET_ERROR_THRESHOLD_PERCENT = 20  # percent of invalid dataset objects/images which will cause job failure\n",
    "PERCENT_100 = 100\n",
    "MIN_TEST_TRAIN_OVERLAP_PERCENT = 50  # minimum overlap percent between test and train labels/classes\n",
    "MIN_BB_DIMENSION = 1.0  # lines with BB width/height smaller than this will be skipped\n",
    "\n",
    "# S3 Batch constants\n",
    "BATCH_MANIFEST_FORMAT = \"S3BatchOperations_CSV_20180820\"\n",
    "BATCH_REPORT_FORMAT = \"Report_CSV_20180820\"\n",
    "BATCH_COPY_PREFIX = \"/copy\"  # output of batch copy\n",
    "BATCH_VALIDATE_PREFIX = \"/image_validation\"  # output of image validation\n",
    "BATCH_REPORT_PREFIX = \"/report\"\n",
    "BATCH_REPORT_MANIFEST_JSON = \"manifest.json\"\n",
    "BATCH_REPORT_JOB_PREFIX = \"job\"\n",
    "TASK_SUCCEEDED = \"succeeded\"\n",
    "TASK_FAILED = \"failed\"\n",
    "\n",
    "# State machine result keys\n",
    "BATCH_COPY_RESULT = \"BatchCopyResult\"\n",
    "BATCH_COPY_JOBS = \"BatchCopyJobIds\"\n",
    "IMAGE_VALIDATION_RESULT = \"ImageValidationResult\"\n",
    "IMAGE_VALIDATION_JOBS = \"ImageValidationJobIds\"\n",
    "INPUT_DATASET_SIZE = \"DatasetSize\"  # total input dataset size (includes invalid lines/images)\n",
    "VALID_TRAINING_DATASET_SIZE = \"ValidTrainingDatasetSize\"  # valid images in training set\n",
    "VALID_TESTING_DATASET_SIZE = \"ValidTestingDatasetSize\"  # valid images in testing set\n",
    "\n",
    "# Image validation lambda task status\n",
    "IMAGE_VALIDATION_PERMANENT_FAILURE = \"PermanentFailure\"\n",
    "IMAGE_VALIDATION_SUCCESS = \"Succeeded\"\n",
    "\n",
    "# Lambda constants\n",
    "IMAGE_VALIDATION_FUNCTION = \"ImageValidationFunction\"\n",
    "\n",
    "# Create Model Artifacts Constants\n",
    "MODEL_GRAPH_DICTIONARY = \"ModelGraphDictionary\"\n",
    "MODEL_ZOO_VERSION = \"model_zoo_version\"\n",
    "\n",
    "# Model Zoo constants for techniques and use cases\n",
    "MZOO_TECHNIQUE_FINETUNING = \"FINETUNING\"\n",
    "MZOO_TECHNIQUE_MULTISTAGE = \"MULTISTAGE\"\n",
    "MZOO_TECHNIQUE_INSTANCE_MATCH = \"INSTANCEMATCH\"\n",
    "MZOO_USE_CASE_CLASSIFICATION = \"CLASSIFICATION\"\n",
    "MZOO_USE_CASE_DETECTION = \"DETECTION\"\n",
    "\n",
    "# Hardcode training instance type for now\n",
    "TRAINING_INSTANCE_TYPE = \"ml.p3.2xlarge\"\n",
    "\n",
    "# workflow execution output\n",
    "WORKFLOW_OUTPUT = \"WorkflowOutput\"\n",
    "\n",
    "# keys in workflow execution output\n",
    "JOB_STATUS = \"jobStatus\"\n",
    "CHILD_WORKFLOW_EXECUTION_STARTTIME = \"childWorkflowExecutionStartTime\"\n",
    "WORKFLOW_NAME = \"workflowName\"\n",
    "\n",
    "# StepFunction Job Status\n",
    "JOB_STATUS_SUCCEED = \"SUCCEED\"\n",
    "JOB_STATUS_CLIENT_ERROR = \"CLIENT_ERROR\"\n",
    "JOB_STATUS_SERVICE_ERROR = \"SERVICE_ERROR\"\n",
    "\n",
    "# Autosplit lambda response keys\n",
    "IS_AUTOSPLIT_KEY = 'IS_AUTOSPLIT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import jsonlines\n",
    "from jsonlines.jsonlines import InvalidLineError\n",
    "import fastjsonschema\n",
    "#from datetime import datetime\n",
    "\n",
    "#from rekognition_auto_ml_lambda.schemas.schema import Schemas\n",
    "#from rekognition_auto_ml_lambda.utils.constants import SOURCE_REF, METADATA_SUFFIX, LABEL\n",
    "#from rekognition_auto_ml_lambda.utils.constants import ATTRIBUTE_ANNOTATIONS, ATTRIBUTE_IMAGE_SIZE\n",
    "#from rekognition_auto_ml_lambda.utils.constants import METADATA_CLASS_MAP, METADATA_CLASS_NAME, \\\n",
    "#    METADATA_HUMAN_ANNOTATED, METADATA_OBJECTS, METADATA_CONFIDENCE, ANNOTATION_TYPE, LABEL_TYPE_CLASSIFICATION, \\\n",
    "#    LABEL_TYPE_DETECTION, LINE_INDEX, METADATA_TYPE, METADATA_CREATION_DATE, METADATA_CLASS_ID\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class GroundTruthManifestStatistics:\n",
    "    \"\"\"\n",
    "    Manifest statistics\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_line_count = 0\n",
    "        self.source_ref_only_line_count = 0\n",
    "        self.valid_line_count = 0\n",
    "        self.valid_annotations = 0\n",
    "        self.invalid_annotations = 0\n",
    "\n",
    "    def __add__(self, stats):\n",
    "        self.total_line_count += stats.total_line_count\n",
    "        self.source_ref_only_line_count += stats.source_ref_only_line_count\n",
    "        self.valid_line_count += stats.valid_line_count\n",
    "        self.valid_annotations += stats.valid_annotations\n",
    "        self.invalid_annotations += stats.invalid_annotations\n",
    "        return self\n",
    "\n",
    "    def get_total_line_without_source_ref_only_line(self):\n",
    "        # do not include line with image but no annotation in total\n",
    "        # https://sim.amazon.com/issues/REKN-3195\n",
    "        return self.total_line_count - self.source_ref_only_line_count\n",
    "\n",
    "    def __eq__(self, stats):\n",
    "        try:\n",
    "            return self.total_line_count == stats.total_line_count \\\n",
    "                and self.source_ref_only_line_count == self.source_ref_only_line_count \\\n",
    "                and self.valid_line_count == stats.valid_line_count \\\n",
    "                and self.valid_annotations == stats.valid_annotations \\\n",
    "                and self.invalid_annotations == stats.invalid_annotations\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"total line:{0}, source_ref_only:{1}, valid_line:{2}, valid_annotations:{3}, invalid_annotations:{4}\"\\\n",
    "            .format(self.total_line_count, self.source_ref_only_line_count, self.valid_line_count,\n",
    "                    self.valid_annotations, self.invalid_annotations)\n",
    "\n",
    "\n",
    "class GroundTruthManifestValidator:\n",
    "    \"\"\"\n",
    "    Manifest validation class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.source_validator = fastjsonschema.compile(Schemas.SOURCE_SCHEMA)\n",
    "        self.metadata_validator = fastjsonschema.compile(Schemas.METADATA_SCHEMA)\n",
    "        self.attribute_validator = fastjsonschema.compile(Schemas.ATTRIBUTE_SCHEMA)\n",
    "\n",
    "    def _transform_attribute_and_metadata(self, attribute, metadata):\n",
    "        \"\"\"Extract necessary fields\n",
    "        Currently we only retain fields which can fit into OpenImage format used by science training code\n",
    "\n",
    "        :param attribute: attribute info\n",
    "        :param metadata: metadata info\n",
    "        :return: validated label\n",
    "        \"\"\"\n",
    "\n",
    "        valid_label = {}\n",
    "        if LABEL_TYPE_CLASSIFICATION in metadata[METADATA_TYPE]:\n",
    "            # classification\n",
    "            valid_label[METADATA_CLASS_NAME] = metadata[METADATA_CLASS_NAME]\n",
    "            valid_label[METADATA_CONFIDENCE] = metadata[METADATA_CONFIDENCE]\n",
    "            valid_label[METADATA_HUMAN_ANNOTATED] = metadata[METADATA_HUMAN_ANNOTATED]\n",
    "            valid_label[METADATA_CREATION_DATE] = metadata[METADATA_CREATION_DATE]\n",
    "            valid_label[ANNOTATION_TYPE] = LABEL_TYPE_CLASSIFICATION\n",
    "        else:\n",
    "            # detection\n",
    "            valid_label[METADATA_HUMAN_ANNOTATED] = metadata[METADATA_HUMAN_ANNOTATED]\n",
    "            valid_label[METADATA_OBJECTS] = metadata[METADATA_OBJECTS]\n",
    "            valid_label[METADATA_CLASS_MAP] = metadata[METADATA_CLASS_MAP]\n",
    "            valid_label[ATTRIBUTE_ANNOTATIONS] = attribute[ATTRIBUTE_ANNOTATIONS]\n",
    "            valid_label[ATTRIBUTE_IMAGE_SIZE] = attribute[ATTRIBUTE_IMAGE_SIZE]\n",
    "            valid_label[METADATA_CREATION_DATE] = metadata[METADATA_CREATION_DATE]\n",
    "            valid_label[ANNOTATION_TYPE] = LABEL_TYPE_DETECTION\n",
    "\n",
    "        return valid_label\n",
    "\n",
    "    def _perform_deep_annotation_check(self, attribute, metadata, current_manifest_stats, line_number):\n",
    "        \"\"\"Perform deep checks across attribute and its metadata\n",
    "\n",
    "        :param annotation: annotation\n",
    "        :return: annotation if valid, None if invalid\n",
    "        \"\"\"\n",
    "        annotation = self._transform_attribute_and_metadata(attribute, metadata)\n",
    "        if LABEL_TYPE_DETECTION in annotation[ANNOTATION_TYPE]:\n",
    "            # Validate if every bounding box label has a confidence\n",
    "            if len(annotation[ATTRIBUTE_ANNOTATIONS]) != len(annotation[METADATA_OBJECTS]):\n",
    "                current_manifest_stats.invalid_annotations += 1\n",
    "                LOGGER.info(\"line %s: Ground truth annotation does not contain valid confidence objects\", line_number)\n",
    "                print(f\"line {line_number}: Ground truth annotation does not contain valid confidence objects\" )\n",
    "                return None\n",
    "\n",
    "            # Validate if bounding box class_id exists in the class map\n",
    "            for bounding_box in annotation[ATTRIBUTE_ANNOTATIONS]:\n",
    "                if str(bounding_box[METADATA_CLASS_ID]) not in annotation[METADATA_CLASS_MAP]:\n",
    "                    current_manifest_stats.invalid_annotations += 1\n",
    "                    LOGGER.info(\"line %s: Ground truth annotation does not contain valid class_id: %s\",\n",
    "                                line_number, str(bounding_box[METADATA_CLASS_ID]))\n",
    "                    print(f\"line {line_number}: Ground truth annotation does not contain valid class_id: {str(bounding_box[METADATA_CLASS_ID])}\")\n",
    "                    return None\n",
    "\n",
    "        return annotation\n",
    "\n",
    "    def _is_valid_source_ref(self, line, line_number):\n",
    "        \"\"\"\n",
    "        Validate if this line contains a valid source-ref value that matches specified JSONSchema\n",
    "        \"\"\"\n",
    "        if SOURCE_REF not in line:\n",
    "            LOGGER.info(\"line %s: Ground truth annotation does not contain valid image source reference\", line_number)\n",
    "            print(f\"line {line_number}: Ground truth annotation does not contain valid image source reference\")\n",
    "            return False\n",
    "        try:\n",
    "            self.source_validator({SOURCE_REF: line[SOURCE_REF]})\n",
    "        except fastjsonschema.exceptions.JsonSchemaException:\n",
    "            LOGGER.info(\"line %s: Invalid image source reference format\", line_number)\n",
    "            print(f\"line {line_number}: Invalid image source reference format\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _get_attributes_from_line(self, line, line_number):\n",
    "        \"\"\"Validate each GT manifest line entry and create list of annotations\n",
    "\n",
    "        :param line: GT manifest JSON line\n",
    "        :return: Returns image source and list of valid annotations\n",
    "        \"\"\"\n",
    "        # Filter attributes which have metadata\n",
    "        attributes = {k: v for k, v in line.items() if f'{k}-{METADATA_SUFFIX}' in line}\n",
    "        if not attributes:\n",
    "            LOGGER.info(\"line %s: Ground truth annotation does not contain any valid attributes\", line_number)\n",
    "            print(f\"line {line_number}: Ground truth annotation does not contain any valid attributes\")\n",
    "            return None\n",
    "        return attributes\n",
    "\n",
    "    def _source_ref_only(self, line):\n",
    "        # _is_valid_source_ref function already verified the key exist\n",
    "        return len(line.keys()) == 1\n",
    "\n",
    "    def _validate_line(self, line, attributes, line_number, current_manifest_stats):\n",
    "        # Validate attribute and metadata against schema and populate necessary fields\n",
    "        valid_annotations = []\n",
    "        for key, value in attributes.items():\n",
    "            metadata_key = f'{key}-{METADATA_SUFFIX}'\n",
    "            try:\n",
    "                self.attribute_validator({key: value})\n",
    "                self.metadata_validator({metadata_key: line[metadata_key]})\n",
    "                valid_annotation = self._perform_deep_annotation_check(\n",
    "                    line[key], line[metadata_key], current_manifest_stats, line_number)\n",
    "                if valid_annotation:\n",
    "                    valid_annotations.append(valid_annotation)\n",
    "                    current_manifest_stats.valid_annotations += 1\n",
    "            except fastjsonschema.exceptions.JsonSchemaException as exc:\n",
    "                current_manifest_stats.invalid_annotations += 1\n",
    "                LOGGER.info(\"line %s: Invalid annotation for attribute\", line_number)\n",
    "                print(f\"line {line_number}: Invalid annotation for attribute\")\n",
    "                LOGGER.debug(\"line %s: Invalid annotation for attribute: %s due to: %s\", line_number, key, exc)\n",
    "                print(f\"line {line_number}: Invalid annotation for attribute: {key} due to: {exc}\")\n",
    "\n",
    "        return {SOURCE_REF: line[SOURCE_REF], LABEL: valid_annotations, LINE_INDEX: line_number} if valid_annotations \\\n",
    "            else None\n",
    "\n",
    "    def validate_manifest(self, manifest, existing_manifest_stats):\n",
    "        \"\"\"\n",
    "\n",
    "        :param manifest: manifest file\n",
    "        :param existing_manifest_stats: contains the statistics of existing\n",
    "        manifest file\n",
    "        :return: manifest and stats\n",
    "        \"\"\"\n",
    "        sanitized_manifest = []\n",
    "        line_number = existing_manifest_stats.total_line_count + 1\n",
    "        current_manifest_stats = GroundTruthManifestStatistics()\n",
    "        reader = jsonlines.Reader(manifest)\n",
    "        while True:\n",
    "            line = None\n",
    "            try:\n",
    "                line = reader.read(type=dict)\n",
    "            except (InvalidLineError, UnicodeDecodeError) as exc:\n",
    "                LOGGER.debug(\"line %s: Unexpected jsonline error in manifest: %s\", line_number, exc)\n",
    "                print(f\"line {line_number}: Unexpected jsonline error in manifest: {exc}\")\n",
    "            except EOFError:\n",
    "                LOGGER.info(\"line %s: Finished parsing manifest file (not all are necessarily valid)\", line_number)\n",
    "                print(f\"line {line_number}: Finished parsing manifest file (not all are necessarily valid)\")\n",
    "                break\n",
    "\n",
    "            if line and self._is_valid_source_ref(line, line_number):\n",
    "                attributes = self._get_attributes_from_line(line, line_number)\n",
    "                if attributes:\n",
    "                    validated_line = self._validate_line(line, attributes,\n",
    "                                                         line_number, current_manifest_stats)\n",
    "                    if validated_line:\n",
    "                        sanitized_manifest.append(validated_line)\n",
    "                        current_manifest_stats.valid_line_count += 1\n",
    "                elif self._source_ref_only(line):\n",
    "                    current_manifest_stats.source_ref_only_line_count += 1\n",
    "\n",
    "            line_number += 1\n",
    "            current_manifest_stats.total_line_count += 1\n",
    "\n",
    "        return sanitized_manifest, current_manifest_stats + existing_manifest_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 66: Ground truth annotation does not contain valid class_id: 0\n",
      "line 219: Ground truth annotation does not contain valid class_id: 0\n",
      "line 258: Ground truth annotation does not contain valid class_id: 0\n",
      "line 283: Finished parsing manifest file (not all are necessarily valid)\n"
     ]
    }
   ],
   "source": [
    "stats = GroundTruthManifestStatistics()\n",
    "validator = GroundTruthManifestValidator()\n",
    "manif, stats2 = validator.validate_manifest(open('data/train_manifest_5_limit.json', 'r'), stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total line:282, source_ref_only:0, valid_line:279, valid_annotations:279, invalid_annotations:3\n"
     ]
    }
   ],
   "source": [
    "print(stats2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Manifest in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(f'data/train_manifest{manifest_ext}.json', bucket, f\"{reko_manifest_path}/train_manifest{manifest_ext}.json\")\n",
    "s3.upload_file(f'data/test_manifest{manifest_ext}.json', bucket, f\"{reko_manifest_path}/test_manifest{manifest_ext}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reko = boto3.client(\"rekognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ProjectArn': 'arn:aws:rekognition:us-east-1:113147044314:project/construction-object-recognition/1582735151793',\n",
       "  'CreationTimestamp': datetime.datetime(2020, 2, 26, 16, 39, 11, 793000, tzinfo=tzlocal()),\n",
       "  'Status': 'CREATED'},\n",
       " {'ProjectArn': 'arn:aws:rekognition:us-east-1:113147044314:project/vtg-demo-1/1579815781951',\n",
       "  'CreationTimestamp': datetime.datetime(2020, 1, 23, 21, 43, 1, 951000, tzinfo=tzlocal()),\n",
       "  'Status': 'CREATED'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reko.describe_projects()['ProjectDescriptions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project(project_name=\"construction-object-recognition\"):\n",
    "    try:\n",
    "        project = reko.create_project(ProjectName=project_name)['ProjectArn']\n",
    "    except reko.exceptions.ResourceInUseException:\n",
    "        project = 'arn:aws:rekognition:us-east-1:113147044314:project/construction-object-recognition/1582735151793'\n",
    "    return(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = get_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ProjectVersionArn': 'arn:aws:rekognition:us-east-1:113147044314:project/construction-object-recognition/version/construction-object-recognition.2020-02-27T01.20.12/1582762812913',\n",
       "  'CreationTimestamp': datetime.datetime(2020, 2, 27, 0, 20, 12, 913000, tzinfo=tzlocal()),\n",
       "  'Status': 'TRAINING_IN_PROGRESS',\n",
       "  'StatusMessage': 'The model is being trained.',\n",
       "  'OutputConfig': {'S3Bucket': 'custom-labels-console-us-east-1-0435516b92',\n",
       "   'S3KeyPrefix': 'evaluation/arn:aws:rekognition:us-east-1:113147044314:project/construction-object-recognition/1582735151793'},\n",
       "  'TrainingDataResult': {'Input': {'Assets': [{'GroundTruthManifest': {'S3Object': {'Bucket': 'custom-labels-console-us-east-1-0435516b92',\n",
       "        'Name': 'datasets/construction-objects-train/manifests/output/output.manifest',\n",
       "        'Version': 'CiD5meCOk_kuud5Eti4c8IDavb1sw7D1'}}}]}},\n",
       "  'TestingDataResult': {'Input': {'Assets': [], 'AutoCreate': True}}},\n",
       " {'ProjectVersionArn': 'arn:aws:rekognition:us-east-1:113147044314:project/construction-object-recognition/version/construction-object-recognition.2020-02-26T23.42.03/1582756923340',\n",
       "  'CreationTimestamp': datetime.datetime(2020, 2, 26, 22, 42, 3, 340000, tzinfo=tzlocal()),\n",
       "  'Status': 'TRAINING_FAILED',\n",
       "  'StatusMessage': 'Amazon Rekognition experienced a service issue.',\n",
       "  'OutputConfig': {'S3Bucket': 'custom-labels-console-us-east-1-0435516b92',\n",
       "   'S3KeyPrefix': 'evaluation/arn:aws:rekognition:us-east-1:113147044314:project/construction-object-recognition/1582735151793'},\n",
       "  'TrainingDataResult': {'Input': {'Assets': [{'GroundTruthManifest': {'S3Object': {'Bucket': 'custom-labels-console-us-east-1-0435516b92',\n",
       "        'Name': 'datasets/construction-objects-train/manifests/output/output.manifest',\n",
       "        'Version': 'CiD5meCOk_kuud5Eti4c8IDavb1sw7D1'}}}]}},\n",
       "  'TestingDataResult': {'Input': {'Assets': [{'GroundTruthManifest': {'S3Object': {'Bucket': 'custom-labels-console-us-east-1-0435516b92',\n",
       "        'Name': 'datasets/construction-objects-test/manifests/output/output.manifest',\n",
       "        'Version': 'B7LKTyAinO6cMu91SY4QC6CXs8kOHKnI'}}}],\n",
       "    'AutoCreate': False}}}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reko.describe_project_versions(ProjectArn=project)['ProjectVersionDescriptions']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
